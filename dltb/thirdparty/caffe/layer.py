"""
See https://github.com/netaz/caffe2any/blob/master/topology.py for some inspiration.
"""

# standard imports
from functools import wraps

# third party imports
from caffe.proto import caffe_pb2
import google.protobuf.text_format

from ...network import layer
from ...util.array import adapt_data_format
from ...util.array import DATA_FORMAT_CHANNELS_LAST as channels_last


def remove_batch_dimension(shape: tuple) -> tuple:
    """Set the batch dimension to None as it does not matter for the
    Network interface."""
    shape = list(shape)
    shape[0] = None
    shape = tuple(shape)
    return shape

def channels_last(shape_fn):
    """Check whether a shape generated by a function has enough dimensions
    and if so put the channels last.

    Parameters
    ----------
    shape_fn

    Returns
    -------

    """
    @wraps(shape_fn)
    def wrapper(*args, **kwargs):
        shape = shape_fn(*args, **kwargs)
        if len(shape) > 2:
            shape = adapt_data_format(shape, output_format=channels_last)
        return shape
    return wrapper


class CaffeLayer(layer.Layer):
    # Layer types for which the data format has to be converted.
    LAYER_TYPES_TO_CONVERT = {
        'Convolution',
        'Input'
    }

    def __init__(self, network, caffe_layer_obj, caffe_layer_proto):
        super().__init__(network)
        self._caffe_layer_obj = caffe_layer_obj
        self._caffe_layer_proto = caffe_layer_proto
        self.layer_name = caffe_layer_proto.name

    @property
    @channels_last
    def input_shape(self):
        previous_layer_idx = list(self._network._caffenet.blobs.keys()).index(self.layer_name) - 1
        # Input to the first layer is, is the output of the layer, since in
        # Caffe the first layer will always be an Input/Data layer.
        if previous_layer_idx == -1:
            previous_layer_idx = 0
        caffe_input_shape = list(self._network._caffenet.blobs.values())[previous_layer_idx].data.shape
        # Set batch dimension to None as it does not matter for other frameworks.
        input_shape = remove_batch_dimension(caffe_input_shape)
        return input_shape

    @property
    @channels_last
    def output_shape(self):
        caffe_output_shape = self._network._caffenet.blobs[self.layer_name].shape
        output_shape = remove_batch_dimension(caffe_output_shape)
        return output_shape



class CaffeNeuralLayer(CaffeLayer, layer.NeuralLayer):

    def __init__(self, network, caffe_layer_obj, caffe_layer_proto, caffe_act_layer_obj, caffe_act_layer_proto):
        super().__init__(network, caffe_layer_obj, caffe_layer_proto)
        self._caffe_act_layer_obj = caffe_act_layer_obj
        self._caffe_act_layer_proto = caffe_act_layer_proto
        self.activation_layer_name = caffe_act_layer_proto.name

    @property
    def parameters(self):
        return tuple(blob.data for blob in self._caffe_layer_obj.blobs)

    @property
    def num_parameters(self):
        return self._caffe_layer_obj.blobs[0].count + self._caffe_layer_obj.blobs[1].count

    # TODO check whether the weight format has also be adapted to channel last
    @property
    def weights(self):
        return self._caffe_layer_obj.blobs[0].data


    @property
    def bias(self):
        return self._caffe_layer_obj.blobs[1].data

class CaffeStridingLayer(CaffeLayer, layer.StridingLayer):

    def _create_kernel_tuple(self, kernel_size):
        # # If the value is not repeated and therefore and int, the kernel has to be square.
        # if isinstance(kernel_size, int):
        #     return tuple(kernel_size, kernel_size)
        # # Else the message is repeated and the protobuf container implements len.
        # elif len(kernel_size) != 0:
        # If only one side is specified the kernel is square.
        if len(kernel_size) == 1:
            return tuple(kernel_size * 2)
        elif len(kernel_size) == 2:
            return tuple(kernel_size)
        else:
            raise ValueError('Kernel with three sides.')
    # else:
    #     raise ValueError('Strange kernel size.')

class CaffeDense(CaffeNeuralLayer, layer.Dense):
    pass

class CaffeConv2D(CaffeNeuralLayer, CaffeStridingLayer, layer.Conv2D):

    @property
    def strides(self):
        strides = list(self._caffe_layer_proto.convolution_param.stride)
        return self._create_kernel_tuple(strides)


    @property
    def padding(self):
        """
        See http://netaz.blogspot.de/2016/08/confused-about-caffes-pooling-layer.html for the rather complicated situation.
        """
        raise NotImplementedError

    @property
    def kernel_size(self):
        kernel_size = list(self._caffe_layer_proto.convolution_param.kernel_size)
        return self._create_kernel_tuple(kernel_size)

    @property
    def filters(self):
        return self._caffe_layer_proto.convolution_param.num_output

class CaffeMaxPooling2D(CaffeStridingLayer, layer.MaxPooling2D):

    @property
    def strides(self):
        strides = [self._caffe_layer_proto.pooling_param.stride]
        return self._create_kernel_tuple(strides)

    @property
    def pool_size(self):
        kernel_size = [self._caffe_layer_proto.pooling_param.kernel_size]
        return self._create_kernel_tuple(kernel_size)


class CaffeDropout(CaffeLayer, layer.Dropout):
    pass

class CaffeFlatten(CaffeLayer, layer.Flatten):
    def __init__(self, network, caffe_layer_obj, caffe_layer_proto, caffe_out_layer_obj, caffe_out_layer_proto):
        super().__init__(network, caffe_layer_obj, caffe_layer_proto)
        self._caffe_out_layer_obj = caffe_out_layer_obj
        self._caffe_out_layer_proto = caffe_out_layer_proto
        self.out_layer_name = caffe_out_layer_proto.name

    @property
    @channels_last
    def input_shape(self):
        caffe_input_shape = self._network._caffenet.blobs[self.layer_name].data.shape
        # Set batch dimension to None as it does not matter for other frameworks.
        input_shape = remove_batch_dimension(caffe_input_shape)
        return input_shape

    @property
    @channels_last
    def output_shape(self):
        caffe_output_shape = self._network._caffenet.blobs[self.out_layer_name].shape
        output_shape = remove_batch_dimension(caffe_output_shape)
        return output_shape











